{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport keras\nimport cv2\nimport h5py\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import RMSprop, Adam\nimport numpy as np\nimport random\nfrom   tensorflow.keras.preprocessing.image import img_to_array, load_img\nimport os\nfrom glob import glob\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.preprocessing import image","execution_count":43,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/urecasegfire/Train\"))\npath = '../input/urecafire/Train'\nprint(path)","execution_count":44,"outputs":[{"output_type":"stream","text":"['Non-Fire', 'Fire']\n../input/urecafire/Train\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","execution_count":45,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#with tpu_strategy.scope():\nmodel = tf.keras.models.Sequential([\n\ntf.keras.layers.Conv2D(16, (3,3), activation='relu',padding = 'same', input_shape=(240, 240, 3)),\n#tf.keras.layers.MaxPooling2D(2, 2),\n\ntf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n#tf.keras.layers.MaxPooling2D(2,2),\n\ntf.keras.layers.Conv2D(64, (3,3), activation='relu'),\ntf.keras.layers.MaxPooling2D(2,2),\n\ntf.keras.layers.Conv2D(64, (3,3), activation='relu'),\ntf.keras.layers.MaxPooling2D(2,2),\n\ntf.keras.layers.Conv2D(64, (3,3), activation='relu'),\ntf.keras.layers.MaxPooling2D(2,2),\n\ntf.keras.layers.Flatten(),\n\ntf.keras.layers.Dense(1024, activation='relu'),\ntf.keras.layers.Dense(512, activation='relu'),\ntf.keras.layers.Dense(32, activation='relu'),\n\ntf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',\n          optimizer=Adam(lr=0.001),\n          metrics=['accuracy'])\n\n","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()\n\n","execution_count":47,"outputs":[{"output_type":"stream","text":"Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_25 (Conv2D)           (None, 240, 240, 16)      448       \n_________________________________________________________________\nconv2d_26 (Conv2D)           (None, 238, 238, 32)      4640      \n_________________________________________________________________\nconv2d_27 (Conv2D)           (None, 236, 236, 64)      18496     \n_________________________________________________________________\nmax_pooling2d_15 (MaxPooling (None, 118, 118, 64)      0         \n_________________________________________________________________\nconv2d_28 (Conv2D)           (None, 116, 116, 64)      36928     \n_________________________________________________________________\nmax_pooling2d_16 (MaxPooling (None, 58, 58, 64)        0         \n_________________________________________________________________\nconv2d_29 (Conv2D)           (None, 56, 56, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_17 (MaxPooling (None, 28, 28, 64)        0         \n_________________________________________________________________\nflatten_5 (Flatten)          (None, 50176)             0         \n_________________________________________________________________\ndense_20 (Dense)             (None, 1024)              51381248  \n_________________________________________________________________\ndense_21 (Dense)             (None, 512)               524800    \n_________________________________________________________________\ndense_22 (Dense)             (None, 32)                16416     \n_________________________________________________________________\ndense_23 (Dense)             (None, 1)                 33        \n=================================================================\nTotal params: 52,019,937\nTrainable params: 52,019,937\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from kaggle_datasets import KaggleDatasets\n# #print(KaggleDatasets())\n# GCS_PATH = KaggleDatasets().get_gcs_path() # you can list the bucket with \"!gsutil ls $GCS_PATH\"","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # On Kaggle you can also use KaggleDatasets().get_gcs_path() to obtain the GCS path of a Kaggle dataset\n# filenames = tf.io.gfile.glob(GCS_PATH) # list files on GCS\n# dataset = tf.data.TFRecordDataset(filenames)\n# # dataset = dataset.map(...) # TFRecord decoding here...\n","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\ntrain_datagen = ImageDataGenerator(rescale=1/255)\n\n\ntrain_generator = train_datagen.flow_from_directory(\n        '../input/urecasegfire/Train',\n        target_size=(240, 240),  \n        batch_size=128, \n\n        class_mode='binary')","execution_count":50,"outputs":[{"output_type":"stream","text":"Found 7529 images belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"file_path = \"./Final.h5\"\ncheckpoint = ModelCheckpoint(file_path, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\nreduce_on_plateau = ReduceLROnPlateau(monitor=\"loss\", mode=\"min\", factor=0.1, patience=5, verbose=1)\nes = EarlyStopping(monitor='accuracy', mode='max', verbose=1, patience=1)\ncallbacks_list = [ checkpoint, reduce_on_plateau, es]\n\nhistory = model.fit(\n      train_generator,\n      steps_per_epoch=58, \n      callbacks=callbacks_list,\n      #validation_split=0.2,\n      #shuffle = True,\n      epochs=100,\n      verbose=1)","execution_count":null,"outputs":[{"output_type":"stream","text":"Train for 58 steps\nEpoch 1/100\n57/58 [============================>.] - ETA: 0s - loss: 0.5434 - accuracy: 0.7295\nEpoch 00001: accuracy improved from -inf to 0.73125, saving model to ./Final.h5\n58/58 [==============================] - 37s 629ms/step - loss: 0.5415 - accuracy: 0.7313\nEpoch 2/100\n57/58 [============================>.] - ETA: 0s - loss: 0.4149 - accuracy: 0.8126\nEpoch 00002: accuracy improved from 0.73125 to 0.81246, saving model to ./Final.h5\n58/58 [==============================] - 35s 605ms/step - loss: 0.4150 - accuracy: 0.8125\nEpoch 3/100\n57/58 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.8647\nEpoch 00003: accuracy improved from 0.81246 to 0.86475, saving model to ./Final.h5\n58/58 [==============================] - 36s 621ms/step - loss: 0.3249 - accuracy: 0.8647\nEpoch 4/100\n57/58 [============================>.] - ETA: 0s - loss: 0.2337 - accuracy: 0.9061\nEpoch 00004: accuracy improved from 0.86475 to 0.90636, saving model to ./Final.h5\n58/58 [==============================] - 35s 607ms/step - loss: 0.2335 - accuracy: 0.9064\nEpoch 5/100\n57/58 [============================>.] - ETA: 0s - loss: 0.1683 - accuracy: 0.9330\nEpoch 00005: accuracy improved from 0.90636 to 0.93312, saving model to ./Final.h5\n58/58 [==============================] - 36s 618ms/step - loss: 0.1690 - accuracy: 0.9331\nEpoch 6/100\n57/58 [============================>.] - ETA: 0s - loss: 0.1072 - accuracy: 0.9597\nEpoch 00006: accuracy improved from 0.93312 to 0.96001, saving model to ./Final.h5\n58/58 [==============================] - 36s 620ms/step - loss: 0.1068 - accuracy: 0.9600\nEpoch 7/100\n36/58 [=================>............] - ETA: 12s - loss: 0.0933 - accuracy: 0.9671","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/onepictest/1.jpg' \nimg = image.load_img(path, target_size=(240, 240))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nimages = np.vstack([x])\nplt.imshow(img)\nplt.show()\nclasses = model.predict(images, batch_size=10)\nprint(classes[0])\nif classes[0]>0.5:\n    print(  \" is a Fire\")\nelse:\n    print( \" is a No Fire\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}